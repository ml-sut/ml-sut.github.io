{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Personal Data\n",
        "Please fill in your details below to help us keep track of your submission.\n",
        "\n",
        "Student Name: `here`\n",
        "\n",
        "Student ID: `here`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOJyVL9Ww4Tg"
      },
      "source": [
        "### in this notebook you are going to implement Bayes classifier and Logistic  regression. please note that you are not allowed to use implemented libraries for the implementation of this code.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Fie2UFlwqIj"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# You can add new libraries if you like\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovI1i7LQ8v3P"
      },
      "source": [
        "# Metrics (10 pt)\n",
        "\n",
        "- **Accuracy:** measures the overall correctness of the model by calculating the ratio of correctly predicted instances (True Positives and True Negatives) to the total number of predictions. Accuracy measures the proportion of correctly classified instances out of the total instances.\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "- **precision:** focuses on how many of the predicted positive cases were actually positive, making it useful in scenarios where false positives are costly (e.g., spam detection). This is the formula for precision:\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "- **Recall:**, also known as Sensitivity, measures the ability of the model to correctly identify all actual positive cases, which is crucial when missing a positive instance is critical (e.g., medical diagnosis). This is the formalu for calculating recall:\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "- **F1-Score:** is the harmonic mean of precision and recall, providing a balanced measure when there is an imbalance between positive and negative classes, ensuring neither metric dominates the evaluation. This is the formula for calculating f1-score:\n",
        "$$\n",
        "\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "complete the functions bellow to calculate the 4 metrics introduced above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF9f1k2L8trQ"
      },
      "outputs": [],
      "source": [
        "# Metric calculation functions\n",
        "# TODO: complete the codes bellow to calculate the 4 metrics introduced in the above cell.\n",
        "\n",
        "def accuracy(metrics): # ðŸ“£\n",
        "    \"\"\"calculates accuracy\"\"\"\n",
        "    total = None\n",
        "    acc = None\n",
        "    return acc if total != 0 else 0.0\n",
        "\n",
        "def precision(metrics): # ðŸ“£\n",
        "    \"\"\"calculates precision\"\"\"\n",
        "    denominator = None\n",
        "    prec = None\n",
        "    return prec if denominator != 0 else 0.0\n",
        "\n",
        "def recall(metrics): # ðŸ“£\n",
        "    \"\"\"calculates recall\"\"\"\n",
        "    denominator = None\n",
        "    rec = None\n",
        "    return rec if denominator != 0 else 0.0\n",
        "\n",
        "def f1_score(metrics): # ðŸ“£\n",
        "    \"\"\" use the functions defined above to calculate f1-score\"\"\"\n",
        "    prec = None\n",
        "    rec = None\n",
        "    f1 = None\n",
        "    return f1 if (p + r) != 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TdNmzkC-WNR"
      },
      "source": [
        "# Confusion Matrix (5 pt)\n",
        "\n",
        "A **Confusion Matrix** is a table used to evaluate the performance of a classification model in machine learning. It provides a detailed breakdown of the model's predictions compared to the actual labels. The matrix is particularly useful for understanding the types of errors the model is making.\n",
        "\n",
        "The confusion matrix consists of four key metrics:\n",
        "- **True Positive (TP)**: The model correctly predicted the positive class.\n",
        "- **False Positive (FP)**: The model incorrectly predicted the positive class (Type I error).\n",
        "- **True Negative (TN)**: The model correctly predicted the negative class.\n",
        "- **False Negative (FN)**: The model incorrectly predicted the negative class (Type II error).\n",
        "\n",
        "Hereâ€™s a visualization of a confusion matrix:\n",
        "\n",
        "<!-- |                     | Predicted Negative | Predicted Positive |\n",
        "|---------------------|--------------------|--------------------|\n",
        "| **Actual Negative** | TN                 | FP                 |\n",
        "| **Actual Positive** | FN                 | TP                 | -->\n",
        "\n",
        "<!-- ![Confusion Matrix](https://glassboxmedicine.com/wp-content/uploads/2019/02/confusion-matrix.png) -->\n",
        "\n",
        "<img src=\"https://glassboxmedicine.com/wp-content/uploads/2019/02/confusion-matrix.png\" alt=\"Confusion Matrix\" width=\"70%\">\n",
        "\n",
        "\n",
        "## How to Build a Confusion Matrix?\n",
        "\n",
        "To build a confusion matrix, follow these steps:\n",
        "\n",
        "1. **Train a Classification Model**:\n",
        "   - Use a dataset to train a classification model (e.g., logistic regression, decision trees, etc.).\n",
        "\n",
        "2. **Make Predictions**:\n",
        "   - Use the trained model to predict labels for a test dataset.\n",
        "\n",
        "3. **Compare Predictions with Actual Labels**:\n",
        "   - Compare the predicted labels with the actual labels to determine the number of TP, FP, TN, and FN.\n",
        "\n",
        "4. **Create the Matrix**:\n",
        "   - Organize the results into a 2x2 matrix as shown above.\n",
        "\n",
        "\n",
        "\n",
        "now, complete the cell bellow to make a confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cFxnaLv0ze4"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(metrics):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix given the metrics (TP, FP, TN, FN).\n",
        "    Parameters:    metrics (dict): A dictionary containing the following keys:\n",
        "                    - 'TP': True Positives\n",
        "                    - 'FP': False Positives\n",
        "                    - 'TN': True Negatives\n",
        "                    - 'FN': False Negatives\n",
        "    \"\"\"\n",
        "    # TODO: Create the confusion matrix as a 2x2 numpy array # ðŸ“£\n",
        "    conf_matrix = None\n",
        "\n",
        "    plt.figure(figsize=(5, 4))  # Set the figure size\n",
        "    sns.heatmap(\n",
        "        conf_matrix,            # The confusion matrix data\n",
        "        annot=True,             # Annotate the cells with the numeric values\n",
        "        fmt='d',                # Format the annotations as integers\n",
        "        cmap='Blues',           # Use a blue color map\n",
        "        xticklabels=['Predicted Negative', 'Predicted Positive'],  # X-axis labels\n",
        "        yticklabels=['Actual Negative', 'Actual Positive']        # Y-axis labels\n",
        "    )\n",
        "\n",
        "    plt.xlabel('Predicted')  # X-axis label\n",
        "    plt.ylabel('Actual')     # Y-axis label\n",
        "    plt.title('Confusion Matrix')  # Title of the plot\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# example confusion matrix\n",
        "metrics = {\n",
        "    'TP': 50,  # True Positives\n",
        "    'FP': 10,  # False Positives\n",
        "    'TN': 90,  # True Negatives\n",
        "    'FN': 5    # False Negatives\n",
        "}\n",
        "\n",
        "confusion_matrix(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTYXVAOQw_id"
      },
      "source": [
        "# Dataset (15 pt)\n",
        "\n",
        "We are using the Pima Indians diabetes dataset. The objective with this dataset is to create a binary classification model that predicts whether or not an individual has diabetes based on several medical indicators. The target variable is given as Outcome and takes on a value of 1 if the patient has diabetes and 0 otherwise. This is an imbalanced class problem because there are significantly more patients without diabetes than with diabetes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MrR5cMIgw6Tq",
        "outputId": "eea72792-59e1-450d-8ccd-09300adcf337"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          6,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          151,\n          101,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.8841603203754405,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          19.9,\n          31.0,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33132859501277484,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 517,\n        \"samples\": [\n          1.731,\n          0.426,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8e0b0330-4fd3-4089-8462-345b24160521\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e0b0330-4fd3-4089-8462-345b24160521')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e0b0330-4fd3-4089-8462-345b24160521 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e0b0330-4fd3-4089-8462-345b24160521');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b1c1e8a-cf06-45f7-b8d2-5c9a82fc3dcd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b1c1e8a-cf06-45f7-b8d2-5c9a82fc3dcd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b1c1e8a-cf06-45f7-b8d2-5c9a82fc3dcd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run this code to Read in the dataset and display the first 5 lines of the dataframe\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/WillKoehrsen/eecs-491/master/assign/project/diabetes.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYcEMig3E8N"
      },
      "source": [
        "## Impute Missing Values\n",
        "There are several columns in the dataset that contain 0 values, which are not physically possible (e.g., Glucose, BloodPressure, etc.).\n",
        "To correct these values, we can impute the zeros with the median of the column.\n",
        "first\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz_DI4ZPB21J"
      },
      "source": [
        "#### **Theoretical question1:** based on above explanation which of the columns that need to be imputed? (hint: there are 5 columns to impute).\n",
        "answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4resqPgxGHe"
      },
      "outputs": [],
      "source": [
        "# Impute Missing Values\n",
        "# To correct these values, replace the 0s with the median of the respective column.\n",
        "# The median is a robust measure of central tendency and is less affected by outliers, making it a good choice for imputation.\n",
        "# Use the `replace()` function to substitute 0s with the median value for each column.\n",
        "\n",
        "data[''] = None\n",
        "data[''] = None\n",
        "data[''] = None\n",
        "data[''] = None\n",
        "data[''] = None\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwMFC8Wj_MZ8"
      },
      "source": [
        "now check if there are missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvuUGVx-_Ouv"
      },
      "outputs": [],
      "source": [
        "# TODO: check if there are missing (nan) values in columns. you can use isna() function for this.\n",
        "missing = None \n",
        "missing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAw0uIil_R7E"
      },
      "source": [
        "## Normalize data\n",
        "Machine learning algorithms like logistic regression can be affected by the scale of the input features. If one feature has values ranging from 0 to 1 and another has values from 0 to 1000, the algorithm might treat the feature with the larger range as more important, even if itâ€™s not. This can lead to incorrect or poor results. Normalizing the features ensures that all features are treated equally by the algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHN5LGlmFRum"
      },
      "source": [
        "#### **Theoretical question 2:** give 2 more reasons of why should we normalize the data before running an ML algorithm on the data.\n",
        "answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Xecg-x_UDD"
      },
      "outputs": [],
      "source": [
        "# TODO: complete the function bellow for normalizing the dataset.\n",
        "def normalize_data(X: pd.DataFrame):\n",
        "    \"\"\"Normalize features (columns) to have zero mean and unit variance\"\"\"\n",
        "    X = np.array(X)  # Convert DataFrame to numpy array if needed\n",
        "    mean = None  # Column means\n",
        "    std = None    # Column standard deviations\n",
        "    reslut = None\n",
        "    return reslut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_viEQSf_WOI"
      },
      "outputs": [],
      "source": [
        "# Extract the features (X) and the labels (target or y)\n",
        "features = data.drop(columns='Outcome')\n",
        "labels = data.Outcome\n",
        "\n",
        "Pima_X = normalize_data(features)\n",
        "Pima_y = labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NphnFEb2_ljr"
      },
      "source": [
        "## Training and Testing Sets\n",
        "We will be using 200 randomly selected observations for the test set. This leaves 568 observations in the training data from which we will fit our model.\n",
        "\n",
        "you can use the train_test_split method from sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIAmsLt8_k3N"
      },
      "outputs": [],
      "source": [
        "# TODO: complete the line bellow to split the dataset into training and testing sets using 200 observations for testing\n",
        "X_train, X_test, y_train, y_test = None\n",
        "\n",
        "print(type(X_train))\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WauujZb_qWf"
      },
      "source": [
        "## another dataset\n",
        "\n",
        "The **Breast Cancer Wisconsin (Diagnostic) Dataset** from `sklearn.datasets` is a widely used dataset for binary classification tasks. It contains **569 samples** of breast cancer cases, each described by **30 numerical features** computed from digitized images of fine needle aspirate (FNA) of breast masses. The features include the mean, standard error, and worst (largest) values of 10 cell nucleus characteristics (e.g., radius, texture, perimeter, smoothness). The target variable indicates whether the tumor is **malignant (1) or benign (0)**, with **357 benign and 212 malignant** samples. This dataset is useful for training machine learning models in medical diagnosis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lju0B3qE_pfq"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "print(list(data))\n",
        "Cancer_X, Cancer_y = None , None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iID3csSEi-2-"
      },
      "source": [
        "we need to get familiar with the dataset we are using in order to find the best model to fit on this dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y2gX3BCh3Ws"
      },
      "outputs": [],
      "source": [
        "# TODO: use DESCR attribute of the dataset to learn more about the dataset \n",
        "description = None\n",
        "description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gbHlKhX71kp"
      },
      "source": [
        "# Logistic Regression: Theory and Implementation (35 pt)\n",
        "\n",
        "## 1. Introduction to Logistic Regression\n",
        "**Logistic Regression** is a fundamental machine learning algorithm used for **binary classification** problems. Unlike **linear regression**, which predicts continuous values, logistic regression estimates the **probability** of an instance belonging to a particular class.\n",
        "\n",
        "It is widely used in applications such as:\n",
        "- Medical diagnosis (e.g., detecting cancerous vs. non-cancerous tumors)\n",
        "- Spam detection (e.g., spam vs. non-spam emails)\n",
        "- Credit scoring (e.g., loan approval or rejection)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Mathematical Formulation\n",
        "\n",
        "### 2.1. Hypothesis Function (Sigmoid Function)\n",
        "Logistic regression uses the **sigmoid function** (also known as the **logistic function**) to map real-valued inputs into a probability range \\([0,1]\\):\n",
        "\n",
        "$$\n",
        "h(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "where $z$ is the **linear combination** of input features and weights:\n",
        "\n",
        "$$\n",
        "z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = XW + b\n",
        "$$\n",
        "\n",
        "- $X$ is the input feature matrix\n",
        "- $W$ is the weight vector (parameters)\n",
        "- $b$ is the bias term\n",
        "\n",
        "The output \\( h(z) \\) represents the probability that a given instance belongs to the **positive class (1)**.\n",
        "\n",
        "### 2.2. Decision Boundary\n",
        "We classify an instance using a **threshold** (e.g., 0.5):\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } h(z) \\geq 0.5 \\\\\n",
        "0, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Cost Function (Log Loss)\n",
        "To train a logistic regression model, we optimize a **cost function** that measures how well the model fits the data. The **log loss (cross-entropy loss)** is defined as:\n",
        "\n",
        "$$\n",
        "J(W, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log h(z_i) + (1 - y_i) \\log (1 - h(z_i)) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "-  $m$ is the number of training examples\n",
        "-  $y_i$ is the actual label (\\(0\\) or \\(1\\))\n",
        "-  $h(z_i)$ is the predicted probability\n",
        "\n",
        "This function penalizes incorrect predictions:\n",
        "- If $y = 1$ but $h(z)$ is small, the loss is high.\n",
        "- If $y = 0$ but $h(z)$ is close to 1, the loss is high.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Regularization (L2 Regularization)\n",
        "To prevent **overfitting**, we use **L2 regularization** (also called **Ridge Regularization**). This adds a penalty term to the cost function:\n",
        "\n",
        "$$\n",
        "J(W, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log h(z_i) + (1 - y_i) \\log (1 - h(z_i)) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} W_j^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\lambda$ controls the strength of regularization.\n",
        "- Larger $\\lambda$ values reduce overfitting but may increase bias.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Gradient Descent for Parameter Optimization\n",
        "We minimize the cost function using **gradient descent**:\n",
        "\n",
        "$$\n",
        "W_j := W_j - \\alpha \\frac{\\partial J}{\\partial W_j}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
        "$$\n",
        "\n",
        "where $\\alpha$  is the **learning rate** and the gradients are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial W} = \\frac{1}{m} X^T (h - y) + \\frac{\\lambda}{m} W\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_i - y_i)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFOCn6kH3HmS"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, lambda_=0.1):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.lambda_ = lambda_  # L2 regularization strength\n",
        "        self.weights = None  # learnable weights\n",
        "        self.bias = None  # learnable bias\n",
        "        self.loss_history = []\n",
        "        self.classes_ = ['0', '1']\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        # TODO: Clip values to avoid overflow in exp. you can use np.clip()\n",
        "        z = None  # clip z to range (-500, 500)\n",
        "        sigmoid = None # calculate the f(z) where f is the sigmoid function\n",
        "        return sigmoid\n",
        "\n",
        "    def _compute_loss(self, y, h):\n",
        "        \"\"\"\n",
        "        Compute the logistic loss with L2 regularization.\n",
        "        \"\"\"\n",
        "        epsilon = 1e-10  # Small value to avoid log(0)\n",
        "\n",
        "        # TODO: Clip the predicted values (h) to be within [epsilon, 1 - epsilon]\n",
        "        h = None \n",
        "\n",
        "        # TODO: Compute the logistic loss function\n",
        "        loss = None\n",
        "\n",
        "        # TODO: Compute L2 regularization term using self.lambda_ and self.weights\n",
        "        regularization = None\n",
        "\n",
        "        return loss + regularization\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.loss_history = []\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            h = self._sigmoid(linear_model)\n",
        "\n",
        "            loss = self._compute_loss(y, h)\n",
        "            self.loss_history.append(loss)\n",
        "\n",
        "            # TODO: Compute gradients for weights (dw) and bias (db)\n",
        "\n",
        "            # Hint: Compute the gradient for weights (dw) using the formula from Section 5.\n",
        "            # Don't forget to include the L2 regularization term.\n",
        "            dw = None\n",
        "\n",
        "            # Hint: Compute the gradient for bias (db) using the formula from Section 5.\n",
        "            db = None\n",
        "\n",
        "            # TODO: Update the weights and bias using the learning rate\n",
        "            self.weights -= None  # Update weights\n",
        "            self.bias -= None  # Update bias\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Compute the probability of the positive class: use affine mappaing learned.\n",
        "        linear_model = None\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, y_true, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "\n",
        "        if len(self.classes_) == 2:\n",
        "            y_pred = (proba >= threshold).astype(int)\n",
        "        else:\n",
        "            y_pred = np.argmax(proba, axis=1)\n",
        "\n",
        "        # Compute True Positives, False Positives, True Negatives, and False Negatives\n",
        "        result = {\n",
        "            'TP': ,        \n",
        "            ...\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def score(self, X, y):\n",
        "        preds = self.predict(X)\n",
        "        return np.mean(preds == y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwmR5g3V-Lwo"
      },
      "source": [
        "run .fit() function to fit the model on both datasets seperately. at first you should split each dataset into training and test sets and then pass and infer the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHM5qh_v7vS2"
      },
      "outputs": [],
      "source": [
        "X = Pima_X\n",
        "y = Pima_y\n",
        "# TODO: Split into train and test sets\n",
        "X_train_pima, X_test_pima, y_train_pima, y_test_pima = None\n",
        "\n",
        "# TODO: Initialize and train LogisticRegression\n",
        "lr_pima = None\n",
        "lr_pima.fit(X_train, y_train)\n",
        "\n",
        "# TODO: use .predict() function of LogisticRegression class for inference.\n",
        "results = None\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Accuracy:\", accuracy(results))\n",
        "print(\"Precision:\", precision(results))\n",
        "print(\"Recall:\", recall(results))\n",
        "print(\"F1 Score:\", f1_score(results))\n",
        "confusion_matrix(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb7_0QDxBQTc"
      },
      "outputs": [],
      "source": [
        "# TODO: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_bc, y_bc = data.data, data.target\n",
        "\n",
        "# TODO: Split into train and test sets\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = None\n",
        "\n",
        "# TODO: Initialize and train LogisticRegressionCV\n",
        "lr_bc = None\n",
        "lr_bc.fit(X_train_bc, y_train_bc)\n",
        "\n",
        "# TODO: use .predict() function of LogisticRegression class for inference.\n",
        "reset_defaults = None \n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Accuracy:\", accuracy(results))\n",
        "print(\"Precision:\", precision(results))\n",
        "print(\"Recall:\", recall(results))\n",
        "print(\"F1 Score:\", f1_score(results))\n",
        "\n",
        "confusion_matrix(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvud3xVoBsWJ"
      },
      "source": [
        "## AUC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN3keuPiBqYm"
      },
      "outputs": [],
      "source": [
        "# Determine the roc curve and the auc and display\n",
        "def calc_roc(probs, y_test): # ðŸ“£\n",
        "  # Calculate the area under the roc curve\n",
        "  auc = roc_auc_score(y_test, probs)\n",
        "  # Calculate metrics for the roc curve\n",
        "  fpr, tpr, thresholds = roc_curve(y_test, probs) \n",
        "\n",
        "  plt.style.use('bmh')\n",
        "  plt.figure(figsize = (8, 8))\n",
        "\n",
        "  # Plot the roc curve\n",
        "  plt.plot(fpr, tpr, 'b')\n",
        "  plt.xlabel('False Positive Rate', size = 16)\n",
        "  plt.ylabel('True Positive Rate', size = 16)\n",
        "  plt.title('Receiver Operating Characteristic Curve, AUC = %0.4f' % auc,\n",
        "            size = 18)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cloqlmdsBPIr"
      },
      "outputs": [],
      "source": [
        "lr_pred_proba = lr_pima.predict_proba(X_test_pima)\n",
        "calc_roc(lr_pred_proba, y_test_pima)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORag1-XTAOCu"
      },
      "outputs": [],
      "source": [
        "lr_pred_proba = lr_bc.predict_proba(X_test_bc)\n",
        "calc_roc(lr_pred_proba, y_test_bc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KPHmZfqB0gk"
      },
      "source": [
        "# Bayesian Classifier: Theory and Implementation (35 pt)\n",
        "\n",
        "## 1. Introduction to Bayesian Classifier\n",
        "The Bayesian Classifier is based on **Bayes' Theorem**, which provides a way to calculate the probability of a class given some observed data. This method is highly effective for classification problems, especially when the features are conditionally independent given the class. It assumes that the data follows a **Gaussian (normal) distribution** within each class.\n",
        "\n",
        "### Bayes' Theorem\n",
        "Bayes' Theorem is given by:\n",
        "\n",
        "$$\n",
        "P(C_k | X) = \\frac{P(X | C_k)P(C_k)}{P(X)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $P(C_k | X)$ is the posterior probability of class $C_k$ given the feature vector $X$.\n",
        "- $P(X | C_k)$ is the likelihood, which is the probability of the feature vector $X$ given class $C_k$.\n",
        "- $P(C_k)$ is the prior probability of class $C_k$.\n",
        "- $P(X)$ is the evidence, or the total probability of the feature vector across all classes.\n",
        "\n",
        "In the Bayesian Classifier, we need to calculate the likelihood of the feature vector for each class and multiply it by the prior probability of that class.\n",
        "\n",
        "### Gaussian Assumption\n",
        "The likelihood $P(X | C_k)$ is assumed to follow a **Gaussian (Normal) distribution**. This means for each feature $x_i$ in the feature vector $X$, we assume:\n",
        "\n",
        "$$\n",
        "P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mu$ is the mean of feature $x_i$ for class $C_k$,\n",
        "- $\\sigma^2$ is the variance of feature $x_i$ for class $C_k$.\n",
        "\n",
        "### Prior Probability\n",
        "The **prior probability** $P(C_k)$ is simply the fraction of samples that belong to class $C_k$. It can be computed as:\n",
        "\n",
        "$$\n",
        "P(C_k) = \\frac{\\text{Number of samples in class } C_k}{\\text{Total number of samples}}\n",
        "$$\n",
        "\n",
        "## 2. Implementation Overview\n",
        "\n",
        "In the **`__init__`** method, we initialize parameters for the class:\n",
        "\n",
        "- **`var_smoothing`**: A small value added to variances to prevent division by zero or extremely small numbers, which can cause instability.\n",
        "- **`classes_`**: The list of unique classes in the target variable.\n",
        "- **`priors_`**: The prior probabilities for each class.\n",
        "- **`means_`**: The means of each feature for each class.\n",
        "- **`variances_`**: The variances of each feature for each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrzyx0LIClCH"
      },
      "outputs": [],
      "source": [
        "class BayesianClassifier:\n",
        "    def __init__(self, var_smoothing=1e-9):\n",
        "        self.var_smoothing = var_smoothing  # To prevent zero variances\n",
        "        self.classes_ = None\n",
        "        self.priors_ = None\n",
        "        self.means_ = None\n",
        "        self.variances_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ =  # TODO: Identify unique classes \n",
        "        n_classes =  # TODO: Get number of classes \n",
        "        n_features = # TODO: Get number of features\n",
        "\n",
        "        # Initialize arrays to store parameters\n",
        "        self.means_ =  # TODO: Initialize means\n",
        "        self.variances_ =  # TODO: Initialize variances\n",
        "        self.priors_ =  # TODO: Initialize priors\n",
        "\n",
        "\n",
        "        # ðŸ“£\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            X_c =  # TODO: Filter data by class \n",
        "            self.means_[idx, :] =  # TODO: Calculate mean for class \n",
        "            self.variances_[idx, :] =  # TODO: Calculate variance for class \n",
        "            self.priors_[idx] =  # TODO: Calculate prior for class \n",
        "\n",
        "        return self\n",
        "\n",
        "    def _calculate_log_proba(self, X):\n",
        "        log_prob = []\n",
        "        for idx in range(len(self.classes_)):\n",
        "            prior = # TODO: Calculate log of prior\n",
        "            mean = # TODO: Get mean of class\n",
        "            variance =  # TODO: Get variance of class\n",
        "\n",
        "            # Gaussian log probability calculation\n",
        "            log_likelihood = # TODO: Implement Gaussian log-likelihood # ðŸ“£\n",
        "            log_prob.append()  # TODO: Add log-likelihood to log probability\n",
        "\n",
        "        return # TODO: Return the log probabilities\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        log_prob = # TODO: Get log probabilities \n",
        "        # Softmax to get probabilities \n",
        "        prob =  # TODO: Apply softmax\n",
        "        prob /= # TODO: Normalize to get probabilities\n",
        "        return prob\n",
        "\n",
        "    def predict(self, X, y_true, threshold=0.5):\n",
        "        proba =   # TODO: Get predicted probabilities\n",
        "        if len(self.classes_) == 2:  # Handle binary classification\n",
        "            y_pred =  # TODO: Convert to binary prediction # ðŸ“£\n",
        "        else:  # Handle multi-class classification\n",
        "            y_pred = # TODO: Select class with maximum probability # ðŸ“£\n",
        "\n",
        "        # Calculate confusion matrix components\n",
        "        results = {}\n",
        "        results['TP'] = np.sum((y_pred == 1) & (y_true == 1))\n",
        "        results['FP'] = np.sum((y_pred == 1) & (y_true == 0))\n",
        "        results['TN'] = np.sum((y_pred == 0) & (y_true == 0))\n",
        "        results['FN'] = np.sum((y_pred == 0) & (y_true == 1))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def score(self, X, y):\n",
        "        results = self.predict(X, y)\n",
        "        return accuracy(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4AvAv0_DHwW"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize and train BayesianClassifier\n",
        "bc_pima = None\n",
        "bc_pima.fit(X_train_pima, y_train_pima)\n",
        "\n",
        "# TODO: use .predict() function of BayesianClassifier class for inference.\n",
        "results = None\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Accuracy:\", accuracy(results))\n",
        "print(\"Precision:\", precision(results))\n",
        "print(\"Recall:\", recall(results))\n",
        "print(\"F1 Score:\", f1_score(results))\n",
        "confusion_matrix(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfmrvgpxDZCB"
      },
      "outputs": [],
      "source": [
        "lr_pred_proba = bc_pima.predict_proba(X_test_pima)\n",
        "calc_roc(lr_pred_proba[:,1], y_test_pima)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HztVMEWHDaq2"
      },
      "outputs": [],
      "source": [
        "# data = load_breast_cancer()\n",
        "# X, y = data.data, data.target\n",
        "\n",
        "# # Split into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Initialize and train BayesianClassifier\n",
        "bayes_bc = None \n",
        "bayes_bc.fit(X_train_bc, y_train_bc)\n",
        "\n",
        "# TODO: use .predict() function of BayesianClassifier class for inference.\n",
        "results = None\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Accuracy:\", accuracy(results))\n",
        "print(\"Precision:\", precision(results))\n",
        "print(\"Recall:\", recall(results))\n",
        "print(\"F1 Score:\", f1_score(results))\n",
        "confusion_matrix(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJnC0eQTDcgh"
      },
      "outputs": [],
      "source": [
        "bc_pred_proba = bayes_bc.predict_proba(X_test_bc)\n",
        "calc_roc(bc_pred_proba[:, 1], y_test_bc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
